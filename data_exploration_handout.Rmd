---
title: "Data Exploration Handout"
author: "Coleridge"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: 
    margin_references: true
    toc: true
    toc_depth: 4
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
# invalidate cache when the tufte version changes
library(yaml)
library(tidyverse)
library(magrittr)
setwd("/Users/sophierand/Desktop/ada-tdc-2020")
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)

code_snips_file <-"/Users/sophierand/Desktop/ada-tdc-2020/code_snips.yml"
code_snips_list <- yaml::read_yaml(code_snips_file, fileEncoding = "UTF-8")

code_snips <- as.data.frame(matrix(unlist(code_snips_list), nrow=length(code_snips_list), byrow=TRUE))
colnames(code_snips) <- names(code_snips_list[[1]])
```

# Introduction

The goal of the Data Exploration lesson is to provide a framework for describing the relationship between the labor market faced by TANF recipients and their earnings and employment outcomes. Use the analysis to shed light on the current crisis.

We will answer the following questions:

1.  What are different measures of the TANF experience?
1.  What are the earnings and employment outcomes of TANF recipients?  
1.  What are the characteristics of employers likely to hire TANF recipients?  

_Policy Relevance: The TANF program “is designed to help needy families achieve self-sufficiency.” State agencies administering this federal program have an interest in understanding how TANF participation can support long-term, stable employment to achieve these goals. Administrative data on TANF participation joined with administrative data on earnings and employers can be used to describe the labor market outcomes of TANF recipients._

## Lesson Structure

The lesson is given over the course of two days - Concepts of Data Exploration and Applications of Data Exploration. 

```{marginfigure, echo=TRUE}
**Materials**:  
1. Data Exploration Video Lecture  
2. `Data Exploration.ipynb`  
```

This handout mirrors the content in the Data Exploration notebook lesson which you can access only in the Administrative Data Research Facility (ADRF). **There is no sensitive data anywhere in this document.** It is a handy reference to use as you step through the lesson in the ADRF, and to refer back to in the future.

Each section has:   

- <span style="color:green;font-weight: bold; ">Guiding Questions</span>, summarized at the top of the section and dispersed throughout the document in <span style="color:green; ">green</span>.
- <span style="color:blue;font-weight: bold; ">Ask Yourself Questions</span>, quick, bite-sized questions to jog your brain and make sure we're all on the same page. in <span style="color:blue; ">blue</span>.
- <span style="color:red;font-weight: bold; ">Checkpoints</span>,  which are mini-exercises for you to complete, to be reviewed during "in-class" time. They are included in each section in <span style="color:red; ">red</span>.

`r margin_note('these come from the syllabus')`

## Learning Objectives and Outcomes

**Concepts of Data Exploration**

1.   Become familiar with common measures of employment outcomes.  
1.   Begin to think about which measures are most important to their home state’s pilot project.  
1.   Be introduced to specifications of the cohorts.  

**Applications of Data Exploration**`r margin_note('data_exploration.ipynb')`

1.   Become comfortable querying the database with SQL queries in R.  
1.   Become comfortable using the main tables in TANF and UI data.  
1.   Gain skills in creating basic descriptives of the datasets, aggregating data frames.  
1.   Learn to calculate employment outcome metrics (given a pre-defined cohort) which were discussed in **Concepts of Data Exploration**.  



<!-- ## Day 2, Concepts of Data Exploration -->

<!-- `r margin_note('this was all taken from the master syllabus. remove?')` -->

<!-- At the end of this section, participants will have learned of common metrics for employment outcomes and will understand the analytical frame and questions to consider when developing an analysis using such metrics. The lesson will range from covering basic statistics to developing an analytical frame for creating employment metrics. This material will lay the groundwork for the hands-on data exploration tasks covered in Day 3. -->

<!-- _Policy Relevance: Understanding employment trajectories of TANF recipients is the primary focus of this course, and through a lecture and discussion session, participants will understand the primary metrics used to measure employment outcomes.  They will be exposed to the questions that frame the analytical task and yield decisions around which metrics to use. Participants will consider, for example, whether individuals had other TANF experience post-“exit”; whether to look at annual earnings, earnings from dominant employers or all employers, etc. _ -->


<!-- **Learning Objectives:**       -->

<!-- 1.   Become familiar with common measures of employment outcomes.     -->
<!-- 1.   Begin to think about which measures are most important to their home state’s pilot project.   -->
<!-- 1.   Be introduced to specifications of the cohort that they will be working with on Day 3.    -->
<!-- 1.   Be prepared for the hands-on Data Exploration notebook for tomorrow.    -->

<!-- ## Day 3, Applications of Data Exploration -->

<!-- We will apply lessons learned from yesterday’s lecture and begin working with the data in earnest. Participants will learn to query, filter and aggregate the data and create basic descriptive statistics of employment and earnings. Using pre-written code developed to create a cohort of TANF exiters, participants will calculate employment and earnings metrics for the cohort post-exit.  They will also describe the hiring patterns of firms.  -->

<!-- _Policy Relevance: Participants will learn how to generate descriptive statistics on TANF and UI Wage data and calculate employment outcome metrics (e.g. “full quarter employment”)._ -->

<!-- **Learning Objectives:**   -->

<!-- 1.  Become comfortable querying the database with SQL queries in R.    -->
<!-- 1.  Become comfortable using the main tables in TANF and UI data.    -->
<!-- 1.  Gain skills in creating basic descriptives of the datasets, aggregating data frames.   -->
<!-- 1.  Learn to calculate employment outcome metrics (given a pre-defined cohort) which were discussed on Day 2.    -->


# R Setup
Load libraries   
`library(tidyverse)`  

```{marginfigure, echo=TRUE}
It is good practice to put the library name before the function, e.g. `DBI::dbGetQuery(con,query)`  in case the function you are using exists in multiple libraries. However if you're confident that the function you want to use exists in only one of the libraries loaded in your environment, you can skip it.
```

Calling functions from libraries   
`tidyverse::mutate()`  

Getting help on a library or function     
`?strptime`
`?tidyverse`

# Loading Data into R/Jupyter

## Establish Database Connection
We establish a connection with `DBI` and `RPostgreSQL` libraries 

```yaml
drv <- dbDriver("PostgreSQL")
con <- dbConnect(drv
              ,dbname = "postgresql://stuffed.adrf.info/appliedda")
              
              
```

We will use this database connection `con` throughout this lesson. Then we create our first SQL query.

```yaml
query <- "SELECT * 
FROM in_fssa.person_month
LIMIT 20;"

query
```
## Reading in the Data
Queries are run with the function `dbGetQuery` from the `DBI` library. The function expects 2 inoputs - the database connection, `con`, and the `query`.

```yaml
df <- dbGetQuery(con,query)
```


# What's in the database?

```{marginfigure, echo=TRUE}
Every SQL database has a metadata schema called `information_schema`
```
We learn more about what is in the database by inspecting the results of queries  `information_schema`.  Here, we  introduce the `head(df)` function which will print out the first 6 rows of the dataframe `df`.

```yaml
df <- dbGetQuery(con, "SELECT * FROM information_schema.schemata")

head(df)
```

Look at metadata on the columns in tables in the database.

```yaml
query <- "SELECT *
FROM information_schema.columns limit 10;"

dbGetQuery(con,query)
```



```{marginfigure, echo=TRUE}
**SCHEMAS**  
`ada_tdc_2020` - Cohort data. We will primarily query from this schema.    
`in_dwd`- Indiana UI Wage Record data      
`in_fssa` - Indiana TANF claims      
```
Query the schemas we will be using to see what tables they contain. We use the SQL commands `WHERE` and `IN` to filter to just look at our schemas.

```yaml
schema_query <- "SELECT DISTINCT 
table_schema,table_name
FROM information_schema.tables
WHERE table_schema  IN ('in_dwd', 'in_fssa', ;ada_tdc_2020')
limit 10;"

dbGetQuery(con,schema_query)

```

``` {marginfigure, echo = TRUE}
Plaeholders are denoted by `%s` if the expected replacement value is a string; `%d` if the expected replacement value is an integer; run `?sprintf` for more replacement values.
```
We will now pause to _parameterize_ the query we just ran, using a string manipulation function called `sprintf`, which takes a string with placeholders, and their replacement value(s). A parameterized query is more flexible in that you can re-use queries and plug in new parameters (usually in the `FROM` or `WHERE` clause) without rewriting the entire query from scratch. We will use this to look at our two cohorts; and to calculate employer characteristics at different points in time.

First we establish which variables we will want to swap out - here, `schema` and `tbl`.

```yaml
schema <-"in_fssa"
tbl <- "person_month"
```
Then, we write a `base_query` with placeholders (denoted by `%s`) that indicate to the `sprintf` function where the replacement values go.

```yaml
base_query <- "SELECT * 
FROM information_schema.columns
WHERE table_schema = '%s' AND table_name = '%s'
limit 20;"
```

Finally, we use the `sprintf` function to create the query string, which we run like usual with `dbGetQuery`.

```yaml
query <- sprintf(base_query, schema,tbl)

dbGetQuery(con,query)
```

```{marginfigure, echo=TRUE}
**Checkpoint 1 Learning Objectives**:     
Become comfortable updating values in a parameterized function by changing the value of `tbl`; gain familiarity with the format of the `in_fssa.case_month` table.
```

### <span style="color:red">CHECKPOINT 1</span>

Get column metadata on the `case_month` table.

With the parameterized query we just used,

<ol type="a">
  <li>Update the `tbl` variable to the case level table</li>
  <li>Generate the new query</li>
  <li>Run the new query</li>
</ol>

_Solution_
1a
```yaml
`r code_snips$code[code_snips$title=="checkpoint1a_solution"]`

```

1b
```yaml
`r code_snips$code[code_snips$title=="checkpoint1b_solution"]`

```

1c
```yaml
`r code_snips$code[code_snips$title=="checkpoint1c_solution"]`

```


# Individual
## Summary Statistics: TANF

<span style="color:green;font-weight: bold;">Guiding Questions:</span>  

For individuals who had a spell ending in 2016 Q4, we want to know:    

1.  What is the distribution of spell lengths amongst individuals in our cohort?
1.  What is the spell length at the 25th percentile?   
1.  What are the spell lengths at the 10th, 25th, 50th, 75th and 90th percentiles?

```{marginfigure echo = TRUE}
See the [Table Creation Handout](table_creation_handout.html){target="_blank"} for more on how the cohorts were created.
```


**Tables:**
These calculations rely on `ada_tdc_2020.tanf_<year>q<quarter>` which contain data on the TANF spell which ended in the quarter specified in the cohort name. 

First, a quick look at the `in_fssa.person_month` table, from which our cohort originates. 

```yaml
`r code_snips$code[code_snips$title=="tanf_person_preview"]`

```

Now let's look at the 2016 Q4 cohort. This is a cohort of individuals who were primary recipients of TANF and had a spell ending in the fourth quarter of 2016. 

```yaml
`r code_snips$code[code_snips$title=="tanf_cohort_preview"]`

```
```{marginfigure, echo=TRUE}
**Checkpoint 2 Learning Objectives**:     
Practice using SQL functions `LIMIT`, `WHERE`, and `COUNT`.

```
#### <span style="color:red">CHECKPOINT 2</span>


Read in the 2009 Q1 TANF Cohort.</span>

<ol type="a">
  <li>Read in the first 20 rows of `ada_tdc_2020.tanf_2009q1`</li>
  <li>Look at records where recipients had a spell ending in February 2009 (Hint: use `WHERE` and `rep_month`)</li>
  <li>Count the number of distinct individuals in the file (Hint: `count(distinct(ssn))`</li>
</ol>

    
_Solution:_

2a
```yaml
`r code_snips$code[code_snips$notebook == "data_exploration" & code_snips$title=="checkpoint2a_solution"]`

```
2b
```yaml
`r code_snips$code[code_snips$notebook == "data_exploration" & code_snips$title=="checkpoint2b_solution"]`

```
2c
```yaml
`r code_snips$code[code_snips$notebook == "data_exploration" & code_snips$title=="checkpoint2c_solution"]`
```


<span style="color:blue;font-weight: bold; font-style: italic;">_**Ask Yourself: **_</span> What is the difference in the unit of analysis between the TANF claims in `in_fssa.person_month` and the spells in `ada_tdc_2020.tanf_2016q4`? Inspect the outputs of `in_df` and `cohort_df`.

_Answer:_ `in_fssa.person_month` has monthly TANF transactions (disbursement of benefits) and the unit of analysis is `person-month` while `ada_tdc_2020.tanf_2016q4` has data on the most recent spell  (the spell that qualified the individual to be in the cohort).


<span style="color:blue;font-weight: bold; font-style: italic;">_**Ask Yourself: **_</span>  Which of the following are true, where $\ num_{ssn,tanf}$ = # persons in `in_fssa.person_month`, and $\ num_{ssn,cohort}$ = # persons in either of the cohorts?

1. $\ num_{ssn,tanf} > num_{ssn,cohort}$  
1. $\ num_{ssn,tanf} < num_{ssn,cohort}$   
1. $\ num_{ssn,cohort} = num_{rows,cohort}$   
1. $\ num_{ssn,cohort} \leq num_{rows,cohort}$  


_Answer:_ 1, 2

<span style="color:green; ">What is the distribution of spell lengths amongst individuals in our cohort?</span>

The question is asking us about spell lengths, so we aggregate to count the number of persons with a spell of a given length.

```{marginfigure echo = TRUE} 
The column `tanf_spell_months` has the number of months in that spell.
```

```yaml
`r code_snips$code[code_snips$title=="spell_length_dist_query"]`

`r code_snips$code[code_snips$title=="spell_length_dist"]`

```

We can use `min` and `max` to look at the range of values. We could have done this in SQL also, but since we have the dataframe handy, let's run these quick R commands.

```{marginfigure echo = TRUE} 
We use the `$` sign notation to specify the `length` column, which is the column we're looking for descriptives on. 
  
```

```yaml
min(spell_count_df$length)
max(spell_count_df$length)

```


<span style="color:green; ">What is the spell length at the 25th percentile?</span>


We use the `SQL` function `percentile_cont` to calculate spell lengths at the 25th percentile. There are R functions, like `quantile`, that can be used to calculate percentiles, but we conduct the manipulation with SQL to preserve compute resources.

```yaml
`r code_snips$code[code_snips$title=="spell_length_25tile"]`

```

<span style="color:green; ">What are the spell lengths at the 10th, 25th, 50th, 75th and 90th percentiles?</span>

We can extend the query to calculate other percentile values, and use `unnest` function to see the percentile value with the percentile.

```yaml
`r code_snips$code[code_snips$title=="spell_length_percentiles"]`

```

```{marginfigure, echo=TRUE}
**Checkpoint 3 Learning Objectives**:     
Practice writing a parameterized query and use it to generate data for the comparison cohort. 
```

#### <span style="color:red;">CHECKPOINT 3</span>
Calculate spell lengths at different percentiles for the 2009 Q1 Cohort.

Modify the query used to calculate spell length at the 10th, 25th, 50th, 75th and 90th percentile for the cohort exiting TANF in Q1 of 2009 (`ada_tdc_2020.tanf_2009q1`)

<ol type="a">
  <li>Parameterize the query with `sprintf`, adding a placeholder for the table.</li>
  <li>Regenerate the query for the 2009 Q1 cohort and retrieve new results from the database.</li>
  </li>
</ol>

_Solution_   

3a
```yaml
`r code_snips$code[code_snips$notebook == "data_exploration" & code_snips$title=="checkpoint3a_solution"]`

```
3b
```yaml
`r code_snips$code[code_snips$notebook == "data_exploration" & code_snips$title=="checkpoint3b_solution"]`

```

## Summary Statistics: Wages

<span style="color:green;font-weight: bold;">Guiding Questions:</span>  

We are interested in the earnings and employment characteristics of TANF leavers in their 1st and 5th quarters post-exit.
```{marginfigure echo = TRUE}
Introduce `sprintf` function for making queries flexible by creating a placeholder for `yr_quarter` and the table name to calculate distribution of earnings in both cohorts, 1st and 5th quarter post exit   
```
    
    
1.  What is the shape/form of the wage tables?  
1.  How many total jobs did leavers hold in the 1st and 5th quarter post-exit?   
1.  How many individuals were working those jobs?  
1.  What was the distribution of wages in the 1st quarter post-exit?  
1.  What is the employment and earnings (number of jobs, total wages) of individuals in our 2016 Q4 cohort in the quarter after exit?
1.  What was the proportion of individuals in our TANF cohorts with a job during the year after their exit? Proportion with a stable job?
1.  How many individuals in our cohorts do not appear in the wage records during the 8 quarters after their exit?


**Tables:**
These calculations rely on `ada_tdc_2020.tanf_wage_<year>q<quarter>` which contain wage records for individuals in the corresponding cohort from the quarter of exit until 8 quarters post-exit.
`r margin_note('we may want to talk over this one/confirm. based on 2019 tdc')`

<span style="color:green; "> What is the shape/form of the wage tables?  </span>

```yaml
`r code_snips$code[code_snips$title == "wage_cohort_preview"]`

```
<span style="color:green; "> How many total jobs did leavers from the 2016 Q4 cohort hold in the 1st and 5th quarter post-exit?</span>

```{marginfigure, echo=TRUE}
Aggregate the total number of jobs with `count(*)`, since each row is a job. Group by 1st and 5th quarter after exit (Q1 2017 and Q1 2018, respectively).
```

```yaml
`r code_snips$code[code_snips$title == "count_jobs_by_q_after_exit"]`

```

<span style="color:green; ">How many individuals were working those jobs?</span>
```{marginfigure, echo=TRUE}
Aggregate the total number of persons with `count(distinct(ssn))`, since each row is a job and any individual ssn may have multiple jobs. Group by 1st and 5th quarter after exit (Q1 2017 and Q1 2018, respectively).
```

```yaml
`r code_snips$code[code_snips$title == "count_persons_by_q_after_exit"]`

```

<span style="color:green; ">What is the distribution of wages in the 1st quarter after exit?</span>

```{marginfigure, echo=TRUE}
Calculate percentiles at the 10th, 25th, 50th, 75th and 90th percentiles using the PostgreSQL function `percentile_cont` in the 1st quarter after exit. 
```

```yaml
`r code_snips$code[code_snips$title == "wage_percentiles"]`

```
```{marginfigure, echo=TRUE}
**Checkpoint 4 Learning Objectives**:     
Create a parameterized query with placeholders for different data types. Using the query, compare distribution of earnings in different cohorts and quarters after exit.
```

#### <span style="color:red;">CHECKPOINT 4</span>

We just ran a query to calculate the distribution of wages for the 2016 Q4 cohort in the 1st quarter after their exit. Now, modify the query we just ran, calculating distribution of earnings in a quarter after exit, by adding placeholders for the **table**, **year** and **quarter** with the `sprintf` function to make it easy to re-run the query with a different cohort or different post-exit quarter. Pay attention to the data type for your placeholders - while **table** is a string, the others are integers.

Modify the queries for these measures:
<ol type="a">
  <li>Modify the distribution of wages query with `sprintf`, adding placeholders for `table_name` and `post_exit_quarter`</li>
  <li>Using modified query you just made, recreate and run queries for:</li>
  <ul>
  <li>5th quarter post-exit, 2016 Q4 Cohort</li>
  <li>1st quarter post-exit, 2009 Q1 Cohort</li>
  <li>5th quarter post-exit, 2009 Q1 Cohort</li>
  </ul>
</ol>

_Solution_

```{marginfigure, echo=TRUE}
Note that `%d` is used as a placeholder for integer values. Run `?sprintf` for more.
```

4a
```yaml
`r code_snips$code[code_snips$notebook == "data_exploration" & code_snips$title=="checkpoint4a_solution"]`

```

4b, 5th quarter post-exit, 2009 Q1 Cohort

```yaml
`r code_snips$code[code_snips$notebook == "data_exploration" & code_snips$title=="checkpoint4a_solution"]`

```

From now on, we will parameterize all SQL queries where we can, since the goal of our analysis is to compare two cohorts at different points after they exit TANF.

<span style="color:green; ">What is the employment and earnings (number of jobs, total wages) of individuals in our 2016 Q4 cohort in the quarter after exit?</span>

Calculate percentiles on number of jobs and total wages per person in a given quarter post-exit. 
<!--In the notebook, we do this only for the first 100 rows to reduce computational power needed. We filter to look at the 1st quarter post-exit with the `year` and `quarter` columns (intead of `job_yr_q`) to demonstrate use of the `%d` placeholder. -->


```yaml
`r code_snips$code[code_snips$title=="num_wage_jobs_by_ssn_input"]`

`r code_snips$code[code_snips$title=="num_wage_jobs_by_ssn_base_query"]`

`r code_snips$code[code_snips$title=="num_wage_jobs_by_ssn_query"]`

```

We just saw a non-random sample of the number of jobs per person. Now, we calculate a distribution to give us a fuller picture.`r margin_note('i did a distribition on this - it wasnt in the tdc 2019 notebook')`

<span style="color:green; ">What is the distribution of employment and earnings (number of jobs, total wages) of individuals in our cohorts?</span>


```yaml
`r code_snips$code[code_snips$title=="dist_wages_num_jobs_input"]`

`r code_snips$code[code_snips$title=="dist_wages_num_jobs_base_query"]`

`r code_snips$code[code_snips$title=="dist_wages_num_jobs_query"]`

```

Now we turn to looking at the participation of our TANF cohorts in the job market. 

<span style="color:green; ">Among TANF leavers in our cohort, what was the proportion who had a job at any time during the 8 quarters post-exit?</span>  

First we calculate the number of individuals who had any employment in the wages. This will be the numerator. 
```yaml
`r code_snips$code[code_snips$title=="num_tanf_employed"]`

```

Then we calculate the denominator, # individuals in the exit cohort, to find the proportion amongst all TANF leavers. 
```yaml
`r code_snips$code[code_snips$title=="num_tanf"]`

`r code_snips$code[code_snips$title=="rate_tanf_employed"]`

```

Now we will repeat these proportion calculations, but look at the _stably employed_. What do we mean when we say stable employment? There are a number of measures that speak to the stability of employment: full quarter employment, full year employment, or employment earning over a certain threshold. Here, we will calculate full quarter employment, meaning employment at the same firm for 3 consecutive quarters. This is a `CROSS JOIN` and in order to parameterize, we can still use `sprintf` but the placeholder is `%1$s` instead of `%s`. We also use the `rep` function to generate the input to `sprintf`. See [table_creation](table_creation.html) for more. 

<span style="color:green; ">Among all TANF leavers, what was the proportion of TANF individuals with a stable job at any time during the 8 quarters post-exit? Among employed TANF leavers, what was the proportion who were stably employed?</span>
First we calculate the number of individuals who had three consecutive quarters of employment with the same employer. This will be the numerator. 
```yaml

`r code_snips$code[code_snips$title=="num_stable_employed"]`

```

Then we use the values calculated earlier as the denominators. First, we calculate the proportion amongst all employed TANF leavers. 

```yaml
`r code_snips$code[code_snips$title=="rate_stable_employed"]`

```

Then we calculate the proportion amongst all TANF leavers. 
```yaml

`r code_snips$code[code_snips$title=="rate_stable_tanf"]`

```


<span style="color:red; font-weight: bold;">CHECKPOINT 4:</span>
We just calculated full quarter employment for the 2016 Q4 cohort. Now calculate it for the 2009 Q1 cohort.
`r margin_note('come up with a better checkpoint?')`

# Employer
## Summary Statistics: Employer Characteristics

**Guiding Questions:**  

We want to look at characteristics of firms that employ TANF leavers and all employers. TANF Employers are those employers who had at least one ssn from our TANF cohorts in their wage records. 

1.  What is the employment (total, stable) of firms employing TANF leavers?
1.  What is the payroll/earnings (total, average, stable) of these firms?
1.  In which industries do leavers find stable employment?
1.  Other measures of (turnover, earnings of workers in different percentiles)
    
First let's take a quick look the form of the employer characteristics tables:

<span style="color:green; ">What is the employment (total and stable) of employers?</span>
<span style="color:green; ">In which industries do leavers find stable employment?</span>


# SCRAP
```{marginfigure ECHO = TRUE}
**Columns**
  
`uiacct` - firm identifier 
`naics_3_digit` - NAICS code indicating industry type
`size` - # unique SSNs employed at the firm in the first quarter post-exit
```


We begin looking at characteristics of the firms that employ TANF leavers in our cohorts. The tables we will read from are `ada_tdc_2020.tanf_2016q4_empl` and `ada_tdc_2020.tanf_2009q1_empl`.

<span style="color:green; ">In which industries do leavers find stable employment?</span>

```yaml
cohort <- "2016q4"

base_query <- "SELECT *
FROM ada_tdc_2020.tanf_%s_empl
limit 100;


query <- sprintf(base_query, cohort)

df <- dbGetQuery(con, query)
```

We will answer this by aggregating with the `group_by` and `count` functions from `dplyr` (part of the `tidyverse`). This is our first time aggregating in R - all previous aggregations were done in SQL.

Pipe operator `%>%`

```yaml
df %>% dplyr::group_by(naics_3_digit) %>% count()
```
<span style="color:green; ">How many individuals does the firm employ?</span>

Again we will answer this using R - specifically the `quantile` function from the `stats` library.

```yaml
stats::quantile(df$size, probs = c(0.1,0.25,0.5,0.75,0.9))

```
**I'm pretty sure the remaining questions below do NOT go in data exploration - instead, ML prep like OSU**
1.  How many individuals does the firm employ? Total? `r margin_note('julia included a question about how many ppl does the firm stably employ - just want to make sure that belongs in data exploration. confirm that these earnings measures for the firm side should be here - i think they are in ML prep in OSU')`
1.  What are the wages paid by the firm? Calculate total, average and stable wages.   
1.  Other measures (turnover, earnings of workers in different industries)    


miscellaneous stuff from notebooks I was referencing  


- for how long have they been recieving benefits?  
- caan we find out how much each individual was making and how many jobs they had in 2016 Q4?
- what was the proportion of TANF individuals with a stable job in 2017?
- how many TANF individuals do not have available wage data in our cohort?

NOte that we dont have a QWI notebook (should we?)
- \# of jobs by post-exit quarter (1st and 5th)   
    - \# persons working those jobs by post-exit quarter (1st and 5th)   
    - Distribution of wages in 1st quarter post-exit   
    - \# of jobs per ssn    
    - Distribution of \# of jobs per ssn      

