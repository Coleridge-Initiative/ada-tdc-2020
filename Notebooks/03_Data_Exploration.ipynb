{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"400\">\n",
    "</center>\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "<center> Julia Lane, Clayton Hunter, Brian Kim, Benjamin Feder, Ekaterina Levitskaya, Tian Lou, Lisa Osorio-Copete. \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "JupyterLab contains a dynamic table of contents that can be accessed by clicking the sixth (second from bottom) icon on the left-hand toolbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In an ideal world, we have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). We'd also have perfect data documentation, with summary statistics and appropriate aggregate measures of everything we'd want to investigate. However, that is hardly ever true, and we have to use our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will discover the datasets we have on the ADRF and use them to answer some questions of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "This notebook will give you the opportunity to spend some hands-on time with the data. We will base our discussions around the following questions:\n",
    "\n",
    "- What does the TANF experience look like? \n",
    "- Where are TANF recipients finding employment? \n",
    "\n",
    "Throughout the notebook, you will be exposed to various programming techniques to help answer these questions. This notebook will form the basis of all the other types of analyses you will do in this class and is a crucial first step for any data analysis workflow. As you work through the notebook, we will have checkpoints for you try out your own code, but you can also think about how you might apply any of the techniques and code presented with other datasets as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets We Will Explore In This Notebook\n",
    "- **Indiana TANF data**: Data on TANF cases as well as those associated with TANF cases in Indiana.\n",
    "- **Indiana Unemployment Insurance (UI) Wage data**: Indiana workers' quarterly earnings and employment. \n",
    "\n",
    "You will explore these datasets using a combination of SQL and R, as explained in the handy Leveraging R and SQL [document](leveraging_r_and_sql.md).\n",
    "\n",
    "**This notebook will provide an introduction and examples for:**\n",
    "\n",
    "- How to create new tables from the larger tables in database (sometimes called the \"analytical frame\")\n",
    "- How to explore different variables of interest\n",
    "- How to create aggregate metrics\n",
    "- How to join tables\n",
    "- How to generate descriptive statistics to describe a specific cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "With our SQL queries, we will:\n",
    "\n",
    "- Learn about rows and columns in the data with basic queries using `SELECT`, `UNIQUE` and `ORDER BY` \n",
    "- Select subsets of tables from the database with `WHERE`\n",
    "- Aggregate data over groups with `GROUP BY`\n",
    "\n",
    "And we will use R to:\n",
    "\n",
    "- Find measures broken down by group using `group_by` and `summarize`\n",
    "- Sort values with `arrange` and `desc`\n",
    "- Create new variables with `mutate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R Setup\n",
    "\n",
    "Before we can use R functions in some specific packages that are not available in `base` R, we need to load them using the built-in function `library()`. For example, running `library(tidyverse)` loads the `tidyverse` suite of packages.\n",
    "\n",
    "> When you run the following code cell, don't worry about the message below. You'll find out what it's saying later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#database interaction imports\n",
    "library(DBI)\n",
    "library(RPostgreSQL)\n",
    "\n",
    "# data manipulation/visualization\n",
    "library(tidyverse)\n",
    "\n",
    "# scaling data\n",
    "library(scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When in doubt, use shift + tab to read the documentation of a method. Full documentation can be printed with `?<package/function_name>`, e.g. `?tidyverse/ggplot` or `?sprintf`.__ Do not worry about memorizing the information in the help documentation - you can always run this command when you are unsure of how to use a function.\n",
    "\n",
    "> Certain functions exist across multiple packages (e.g. the function `lag` exists in both the `dplyr` and `stats` package - also noted in the message yielded from `library(tidyverse)`. When calling a function, you can put the package name first to ensure that you are using the right one. For example, `dplyr::lag` or `stats::lag` calls the `lag` function from `dplyr` or `stats`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see help documentation for sprintf\n",
    "?sprintf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Since we are working with the PostgreSQL database `appliedda` in this course, we will demonstrate how to use R to read data from a relational database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Connection to the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the database connection using the `DBI`  and `RPostgreSQL` libraries. Each time you create a new notebook in this course, make sure you copy the following code chunk so you can connect to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Database Connection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an RPostgreSQL driver\n",
    "drv <- dbDriver(\"PostgreSQL\")\n",
    "\n",
    "# connect to the database\n",
    "con <- dbConnect(drv,dbname = \"postgresql://stuffed.adrf.info/appliedda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate Data Query\n",
    "\n",
    "This part is similar to writing a SQL query in DBeaver. Depending on the data we are interested in, we can use different queries to pull different data. In this example, we will pull in 20 rows of person-level TANF spells data, which is stored in the `person_month` table inside the `in_fssa` schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create a query as a `character` string object in R__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query character string\n",
    "query <- \"\n",
    "SELECT *\n",
    "FROM in_fssa.person_month\n",
    "LIMIT 20;\n",
    "\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use `LIMIT` to read in only the first 20 rows because we're just looking to preview the data and we don't want to eat up memory by reading a huge data frame into R. \n",
    "\n",
    "> `LIMIT` provides one simple way to get a \"sample\" of data; however, using `LIMIT` does **not provide a _random_** sample. You may get different samples of data than others using just the `LIMIT` clause, but it is just based on what is fastest for the database to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print with newlines instead of /n\n",
    "writeLines(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data \n",
    "\n",
    "Now we can use `con` and `query` as inputs to `dbGetQuery()` to read the data into an R Data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data frame and assign to df\n",
    "df <- dbGetQuery(con,query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see first few rows of df\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the Database?\n",
    "\n",
    "We will start our exploration by looking at what is in the database. We will find the list of schema names in the database, the list of tables in these schemas, and the list of columns in these tables.\n",
    "\n",
    "\n",
    "We will get this information from a built-in schema called `information_schema`, which provides metadata on the database. The `information_schema` schema has tables `tables`, `schemata`, and others, as you will see.\n",
    "\n",
    "In general, as in DBeaver, database schemas and tables are denoted as `<schema_name>.<table_name>`. \n",
    "\n",
    "You will query the `information_schema` tables: \n",
    "\n",
    "- `information_schema.tables` for metadata on tables   \n",
    "- `information_schema.schemata` for metadata on schemas   \n",
    "- `information_schema.columns` for metadata on columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# see 5 of the available schemas\n",
    "query <- \"\n",
    "SELECT DISTINCT * \n",
    "FROM information_schema.schemata\n",
    "limit 5;\n",
    "\"\n",
    "dbGetQuery(con,query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, in this class you have access to read in data from the following schemas: `public`,`in_dwd`, `in_fssa`, and `ada_tdc_2020`. You only have write access to the `ada_tdc_2020` schema.\n",
    "\n",
    "Let's see a few of the tables that exist in the `in_fssa` schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see five tables in the in_fssa schema\n",
    "qry <- \"\n",
    "SELECT DISTINCT \n",
    "table_schema, table_name \n",
    "FROM information_schema.tables \n",
    "WHERE table_schema IN ('in_fssa')\n",
    "limit 5\n",
    "\"\n",
    "\n",
    "dbGetQuery(con,qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 1: Explore Columns </h3></font> \n",
    "\n",
    "Take a look at the columns in the `person_month` table in the `in_fssa` schema. What are some of the variables that you might be able to use to answer the questions posed in the Learning Objectives?\n",
    "\n",
    "> Refer to the data dictionary on the class website to understand what the different variables refer to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query <- \"\n",
    "SELECT * \n",
    "FROM information_schema.columns \n",
    "where table_schema = __ and table_name = __;\n",
    "\"\n",
    "\n",
    "dbGetQuery(con,query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sprintf`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll quickly cover the `sprintf` funtion, which we'll use to create our first flexible, or *parameterized*, query.\n",
    "\n",
    "`sprintf` is a string manipulation function that enables us to use symbols as placeholders in R so we can interchange values in an expression. In this case, we would like to see the columns in the other schemas and tables provided to us. Rather than rewriting all the queries, we can use `sprintf` to parameterize the queries, making them much more flexible. \n",
    "\n",
    "`sprintf` takes the form:\n",
    "`sprintf(base_string, input_parameter)`\n",
    "\n",
    "The base string should have a placeholder to denote the part of the string that will be replaced by `input_parameter`.  \n",
    "\n",
    "    `%s` - placeholder where the replacement is a string   \n",
    "    `%d` - placeholder where the replacement is an integer\n",
    "\n",
    "Let's take a simple example to see how `sprintf` works to see the variables in the `case_month` table in the `in_fssa` schema using our query from Checkpoint 1.\n",
    "\n",
    "> We will leverage the power of `sprintf` in later notebooks, as it allows us to input values in R into SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Column names in in_fssa.case_month\n",
    "\n",
    "# assign schema and table here\n",
    "schema <- \"in_fssa\"\n",
    "tbl <- \"case_month\"\n",
    "\n",
    "# use base query and assign table_schema and table_name to '%s'\n",
    "base_query <- \"SELECT * \n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = '%s' AND table_name = '%s'\n",
    "limit 5;\"\n",
    "\n",
    "# feed in the new schema and tbl to query\n",
    "query <- sprintf(base_query,schema, tbl)\n",
    "\n",
    "# see query\n",
    "writeLines(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can just use the query the same way as usual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query from database\n",
    "dbGetQuery(con, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Cohort\n",
    "\n",
    "In this section, you will begin to explore TANF individual spell data. As you work through this section, try to ask yourself questions such as: \n",
    "- What variables are you interested in? \n",
    "- What variables do you need to identify the sample you are interested in?\n",
    "- In which table(s) are these variables available? \n",
    "- Are there any missing values in these variables?\n",
    "\n",
    "We will focus on two specific cohorts: primary TANF benefit recipients who had a spell ending during 2016Q4 and those who spells ended during in 2009Q1. A spell is defined as a period in which an individual/household is receiving aid from TANF. \n",
    "\n",
    "The data we will use to define these two cohorts live in two tables in the `in_fssa` schema: `case_month` and `person_month`. As partially indicated by their names, `case_month` contains case-level information, such as start and end dates as well as the number of individuals pertaining to the case, and `person_month` provides individual-level information on every member of the case, such as a primary recipient indicator, some demographic variables, and a handful of case-level measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the `case_month` table. Here, we will try to identify variables we can use to define and subsequently understand our cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see 5 entries of the case_month table\n",
    "qry <- \"\n",
    "SELECT *\n",
    "FROM in_fssa.case_month\n",
    "LIMIT 5;\n",
    "\"\n",
    "\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two columns in particular in `case_month` that we can use to help define our cohorts by their end dates: `rptmn` and `tanf_end`. These variables describe the reporting month/year of the case and if the month/year was the final month of the case, respectively.\n",
    "\n",
    "Let's take a look at just the `rptmn` and `tanf_end` columns in `case_month`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just look at rptmn and tanf_end\n",
    "qry <- \"\n",
    "SELECT rptmn, tanf_end\n",
    "FROM in_fssa.case_month\n",
    "LIMIT 5;\n",
    "\"\n",
    "\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can't retrieve all of the information we need to create our cohort from just `case_month` since we cannot identify the primary recipients for each case. Let's see if we can identify primary recipients by using `person_month`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see person_month\n",
    "qry <- \"\n",
    "SELECT *\n",
    "FROM in_fssa.person_month\n",
    "LIMIT 5;\n",
    "\"\n",
    "\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you spot the `affil` column? This will help us identify the `ssn` of the primary recipient for each case. From there, we can identify each TANF case using the common `caseid` column that exists across the two tables.\n",
    "\n",
    "Also, it turns out that the same `rptmn` and `tanf_end` columns are available in `person_month`, so we can define our cohort just using the `person_month` table. However, when we want to understand the typical TANF experience within our cohort we will want to look at some of the case-level variables from `case_month`, so we will leverage both tables to define our two cohorts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3>Checkpoint 2: Isolate Primary Recipients</h3></font> \n",
    "\n",
    "Read 5 rows of the `person_month` table where the `ssn` corresponds to the primary recipient of the case into R. How did you figure out which value of `affil` is associated with the primary recipient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find five primary recipients\n",
    "query <- \"\n",
    "\n",
    "\"\n",
    "\n",
    "dbGetQuery(con,query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our cohorts\n",
    "\n",
    "We have all the information we need to find our cohorts of primary recipients of TANF benefits who left the program sometime in 2009Q1 or 2016Q4. Let's start by creating our 2016Q4 table with the addition of variables tracking the length of the spell as well as total length of all spells as the primary recipient. In defining our cohort, we will need to specify a few points in our `WHERE` clause:\n",
    "\n",
    "- `affil = '1'`, for primary recipients\n",
    "- `tanf_end = true`, for concluding spells\n",
    "- `substring(month,1,4) = '2016'`, for spells in 2016\n",
    "- `substring(month,5,2) in ('10', '11', '12')`, for spells in October, November, or December\n",
    "\n",
    "> We have also identified some `ssn` values as problematic, so we will not include them in our cohorts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 2016Q4 cohort\n",
    "qry <- \"\n",
    "select ssn, caseid, tanf_start, tanf_end, tanf_spell_months, tanf_total_months, substring(month,1,4) as rep_year, \n",
    "substring(month,5,2) as rep_month, extract(year from dob) as dob_yr\n",
    "from in_fssa.person_month\n",
    "where affil = '1' and \n",
    "tanf_end=true and \n",
    "substring(month,1,4) = '2016' and \n",
    "substring(month,5,2) in ('10', '11', '12') and\n",
    "ssn not in REDACTED\n",
    "\"\n",
    "#read into R as df\n",
    "df <- dbGetQuery(con,qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at df\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given your background knowledge of the TANF program, do you think that the same `ssn` could show up in `df` multiple times? Let's take a look because in this case, we do not want the same `ssn` to appear in multiple rows in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if any ssns appear more than once in df\n",
    "df %>%\n",
    "    count(ssn) %>%\n",
    "    filter(n>1) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, there are some individuals who were the primary recipients of multiple TANF cases that ended in 2016Q4. Since we do not want repeating `ssn` values in understanding employment outcomes (otherwise overweighting these individuals' outcomes), let's take each `ssn`'s most recent exit within this time frame. We can find this by selecting distinct `ssn` values when ordering by exit month. Also, let's include one variable found only in the case-level data: the county of residence. This means that we need to join our cohort with the `case_month` table in the `in_fssa` schema.\n",
    "\n",
    "> Since we only want `ssn`'s where the `caseid` also contains case-level information specific to the `case_month` table, we will use an `inner join`, as opposed to a `left join` or `right join`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2016Q4 cohort with most recent case information\n",
    "qry <- \"\n",
    "SELECT distinct on (a.ssn)\n",
    "a.ssn, a.caseid, a.month, a.tanf_start, a.tanf_end, a.tanf_spell_months, a.tanf_total_months,b.county,\n",
    "substring(a.month,1,4) as rep_year, substring(a.month,5,2) as rep_month, extract(year from dob) as dob_yr\n",
    "FROM in_fssa.person_month a\n",
    "INNER JOIN in_fssa.case_month b \n",
    "on a.caseid = b.caseid\n",
    "WHERE a.affil = '1' and\n",
    "a.tanf_end = TRUE and \n",
    "ssn not in REDACTED and\n",
    "substring(a.month,1,4) = '2016' and \n",
    "substring(a.month,5,2) in ('10','11','12')\n",
    "order by a.ssn, a.month desc;\n",
    "\"\n",
    "\n",
    "#read into R as df\n",
    "df_2016 <- dbGetQuery(con,qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure, let's confirm that there are not any `ssn` values that appear multiple times in `df_2016`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure no ssns appear in multiple rows in df_2016\n",
    "df_2016 %>%\n",
    "    count(ssn) %>%\n",
    "    filter(n>1) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully defined our 2016Q4 cohort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 3: Recreate for 2009Q1 </h3></font> \n",
    "\n",
    "Recreate `df_2016` for our cohort of primary recipients of TANF benefits that concluded during 2009Q1, including the same variables as `df_2016`, and save the cohort as `df_2009`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cohort for 2009Q1\n",
    "qry = \"\n",
    "\n",
    "\"\n",
    "\n",
    "df_2009 = dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TANF Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we leverage our data to better understand the TANF experience? We will make use of our two cohorts, `df_2009` and `df_2016`, to answer these questions:\n",
    "\n",
    "1. How many individuals are in the 2016Q4 cohort?\n",
    "1. What is the age breakdown of this cohort?\n",
    "1. How does the number of individuals vary by county? \n",
    "1. What is the distribution of spell lengths within this cohort?\n",
    "1. What are the spell lengths at the 10th, 25th, 50th, 75th and 90th percentiles?\n",
    "1. Are we seeing a concentration of lengthy spells in specific regions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will walk through how to find these answers in R for our 2016Q4 cohort, and you will replicate the same analysis on `df_2009` in the checkpoint at the end of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Question 1: How many individuals are in the 2016Q4 cohort? </h4></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because each row is a unique ssn, can just get the count of rows\n",
    "nrow(df_2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Question 2: What is the age breakdown of the cohort? </h4></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice two columns in `df_2016`: `rep_year` and `dob_yr`. We can use these two columns to find the age breakdown of our cohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at rep_year and dob_yr\n",
    "df_2016 %>%\n",
    "    select(rep_year, dob_yr) %>%\n",
    "    head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the ages of everyone in our cohort, we can subtract the `dob_yr` from `rep_year`. We can create this new column using `mutate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see example of mutate\n",
    "df_2016 %>%\n",
    "    mutate(age = as.numeric(rep_year) - dob_yr) %>%\n",
    "    select(rep_year, dob_yr, age) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to ages\n",
    "ages <- df_2016 %>%\n",
    "    mutate(age = as.numeric(rep_year) - dob_yr) %>%\n",
    "    select(rep_year, dob_yr, age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(ages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see age breakdowns of our cohort, let's bin the ages into a few groups. We can use `mutute()` with `case_when()` to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create age groups\n",
    "ages <- ages %>%\n",
    "    mutate(age_group = case_when(\n",
    "        age < 18 ~ \"0-17\",\n",
    "        between(age, 18, 39) ~ \"18-39\",\n",
    "        between(age, 40, 59) ~ \"40-59\",\n",
    "        age >= 60 ~ \"60+\"\n",
    "    )\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see counts by age group\n",
    "ages %>%\n",
    "    count(age_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this age breakdown surprise you at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Question 3: How does the number of individuals vary by county?</h4></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at a few rows of `df_2016` while focusing on the `ssn` and `county` values to get a sense of what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(df_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at ssn and county\n",
    "df_2016 %>%\n",
    "    select(ssn, county) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like there's anything crazy (incorrect data types, in particular), going on with either variable, so let's count the number of exiters by county for a few counties using the `count()` function from the tidyverse.\n",
    "\n",
    "> Sometimes, you may come across some \"craziness\" (incorrect data types, weird values, etc.) in the data just by chance. The code above was just a quick check to make sure our `county` variable was of character type, as opposed to a numeric column, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of exiters by county\n",
    "df_2016 %>%\n",
    "    count(county) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to go a bit more in depth, you can sort by the counties with the most amount of individuals in our cohort using a combination of `arrange()` and `desc()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 most popular counties in our cohort\n",
    "df_2016 %>%\n",
    "    count(county) %>%\n",
    "    arrange(desc(n)) %>%\n",
    "    head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we want to match the county codes to the names of the counties, we can load the `tl_2016_us_county` table from the `public` schema into R and join the two data frames.\n",
    "\n",
    "> Indiana's state fips code is 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get county codes, county names, polygons, and centroids of those polygons for the Indiana state\n",
    "qry <- \n",
    "\"SELECT countyfp as county, name\n",
    "FROM public.tl_2016_us_county\n",
    "WHERE statefp = '18'\n",
    "\"\n",
    "#read into R as df\n",
    "counties <- dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see counties\n",
    "head(counties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to SQL's `LEFT JOIN`, one of the `tidyverse` packages, `dplyr`, contains `left_join()` which we can use to match the county codes to their proper names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save top 10 most popular counties in our cohort\n",
    "top_cnty <- df_2016 %>%\n",
    "    count(county) %>%\n",
    "    arrange(desc(n)) %>%\n",
    "    head(10)\n",
    "\n",
    "# left join to county lookup table\n",
    "    left_join(top_cnty, counties, by=\"county\") %>%\n",
    "    select(name, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just from this summary you can get the sense that a decent chunk of our cohort received TANF benefits from a select few counties. In the [Data Visualization](04_01_Data_Visualization.ipynb) notebook, you will visualize the county breakdown of our cohorts using a heatmap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Guiding Question 4: What is the distribution of spell lengths within our cohort? </h4></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two simple options to find a basic numerical distribution:\n",
    "1. Use base R's `summary()` function, not to be confused with the tidyverse's `summarize()`\n",
    "1. Find specific percentiles using `quantile()`\n",
    "\n",
    "We'll work through both of these techniques in Guiding Questions 3 and 4. Since we can answer this question with a generic summary, we will go with our first option here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quick summary of current spell lengths\n",
    "summary(df_2016$tanf_spell_months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Guiding Question 5: What are the spell lengths at the 10th, 25th, 50th, 75th and 90th percentiles?</h4></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this question's percentile requirements, we will use the `quantile()` function, which allows us to specify certain percentiles.\n",
    "\n",
    "> For the purposes of your final project outputs, you will be asked to report \"fuzzy\" percentiles to maintain data confidentiality. This will be covered more in-depth in the [Disclosure Review](Disclosure_Review.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of percentile values for spell lengths\n",
    "quantile(df_2016$tanf_spell_months, c(.1, .25, .5, .75, .9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon first glance, you may find the output from `quantile()` to be a bit hard to read. Luckily, we can use `quantile` in conjunction with the tidyverse's `summarize()` function to output an easy-to-read data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it look a little prettier\n",
    "df_2016 %>%\n",
    "    summarize('.1' = quantile(tanf_spell_months, .1),\n",
    "              '.25' = quantile(tanf_spell_months, .25),\n",
    "              '.5' = quantile(tanf_spell_months, .5),\n",
    "              '.75' = quantile(tanf_spell_months, .75),\n",
    "              '.9' = quantile(tanf_spell_months, .9)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do any of these numbers suprise you? Or is this what you were expecting to see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Guiding Question 6: Are we seeing a concentration of lengthy spells in specific regions?</h4></font> \n",
    "\n",
    "Now, let's look at the counties that have a high proportion of recipients with long spells. For the purposes of this exercise, we will define a long-term stayer as a recipient with a spell longer than the 90th percentile value. Our process will be as follows:\n",
    "- Identify the 90th percentile of the amount of months of these spells\n",
    "- Create an indicator variable using `mutate()` for whether each spell was greater than that of the 90th percentile\n",
    "- Find the proportion of lengthy spells by county\n",
    "\n",
    "First, we will identify the value at the 90th percentile using our code above. This time, we will assign the output to a specific variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign better looking 90th percentile output to percentile\n",
    "percentile <- df_2016 %>%\n",
    "    summarize('.9' = quantile(tanf_spell_months, .9))\n",
    "\n",
    "percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will test some code to make sure we properly create our indicator variable `rel_length` for each spell in our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if code will work to create long vs not long depending on \n",
    "# if spell length is greater than the 90th percentile of all spell lengths\n",
    "# call indicator variable \"rel_length\"\n",
    "df_2016 %>%\n",
    "    select(ssn, county, tanf_spell_months) %>%\n",
    "    mutate(rel_length = ifelse(tanf_spell_months > percentile$'.9', 'long', 'not long')) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that it seems like we can create `rel_length` using this code, let's store `df_2016` with our new indicator variable as a new data frame `lens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign to variable \"lens\"\n",
    "lens <- df_2016%>%\n",
    "    select(ssn, county, tanf_spell_months) %>%\n",
    "    mutate(rel_length = ifelse(tanf_spell_months > percentile$'.9', 'long', 'not long'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, there is not a set function within the `tidyverse` or `base R` to calculate proportions. We still can find proportions, but it just requires us to find the count per group first.\n",
    "\n",
    "Instead of finding the count using the `count()` function as we have in the past, we can find the count using a combination of `group_by()` and `summarize()` here. We will do so because we will need to further manipulate the data frame by each group when we find the proportion of \"long\" values by county.\n",
    "\n",
    "> If you do not `filter()` for just 'long' spells in the code below, you will have two rows per county, one of the proportion of 'long' spells, and the other the proportion of 'not long' spells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find count and proportion of \"long\" by county sorted by highest proportion\n",
    "lens %>%\n",
    "    group_by(county, rel_length) %>%\n",
    "    summarize(n=n()) %>%\n",
    "    mutate(Proportion = n/sum(n)) %>%\n",
    "    ungroup() %>%\n",
    "    filter(rel_length == 'long') %>%\n",
    "    arrange(desc(Proportion)) %>%\n",
    "    # we don't need to see rel_length column\n",
    "    select(-rel_length) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, instead of just looking at the counties with the highest proportion of longer TANF spells, we can find the `summary()` of the proportions amongst all counties in Indiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign to props\n",
    "props <- lens %>%\n",
    "    group_by(county, rel_length) %>%\n",
    "    summarize(n = n()) %>%\n",
    "    mutate(Proportion = n/sum(n)) %>%\n",
    "    ungroup() %>%\n",
    "    filter(rel_length == 'long') %>%\n",
    "    select(-rel_length)\n",
    "\n",
    "# see distribution by county\n",
    "summary(props$Proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 4: Recreate for 2009Q1 </h3></font> \n",
    "\n",
    "Find the answers to the questions at the beginning of this section \"The TANF Experience\" for the 2009Q1 cohort. \n",
    "\n",
    "Do the calculations vary significantly from the ones for our 2016Q4 cohort? Are you finding that those from similiar counties in our 2016Q4 cohort are more likely to have longer TANF spells as those in the 2009Q1 cohort?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many individuals are in each cohort?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does the number of individuals in the two cohorts vary by county? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the distribution of spell lengths within in our cohort?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the spell lengths at the 10th, 25th, 50th, 75th and 90th percentiles?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we seeing a concentration of lengthy spells in specific regions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employment Outcomes of TANF Exiters\n",
    "\n",
    "Now that we have a better grasp of how the TANF experience can be presented through data, we will take a look at post-exit employment outcomes up to one year after exit. Thus, for our 2016Q4 cohort, we will analyze their employment outcomes for 2017Q1-2017Q4. \n",
    "\n",
    "Are members of our cohort finding some sort of sustainable employment where they will not apply for TANF support in the future? Are they finding any employment at all? If so, what are their wages?\n",
    "\n",
    "To find the answers to these questions, we will first match our two TANF leaver cohorts to the Indiana Unemployment Insurance wage records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match Cohort to UI Wage Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at one of Indiana's UI wage records tables, `wage_by_employer`, which is located in the `in_dwd` schema.\n",
    "\n",
    "> There is another set of tables, `wagesums`, which aggregate earnings for each individual by quarter in Indiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Indiana's wage_by_employer table\n",
    "qry = \"\n",
    "select *\n",
    "from in_dwd.wage_by_employer\n",
    "limit 5\n",
    "\"\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `wage_by_employer` table contains wage entries by quarter, employer, and individual, which is perfect for joining it to our cohort. First, we need to create our cohort table in SQL. Luckily, though, just so you all do not have to create temporary tables at the same time, we have already created the 2016 cohort for you in the `ada_tdc_2020` schema using the code below.\n",
    "\n",
    "> When you create temporary or permanent tables in a relational database using an R Kernel, you will need to run `dbExecute()` instead of `dbGetQuery()` since there is no output to be returned when creating tables. `dbExecute()` will create the table as well as provide the number of rows of the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create table ada_tdc_2020.cohort_2016 as \n",
    "    SELECT distinct on (a.ssn)\n",
    "    a.ssn, a.caseid, a.month, a.tanf_start, a.tanf_end, a.tanf_spell_months, a.tanf_total_months,b.county,\n",
    "    substring(a.month,1,4) as rep_year, substring(a.month,5,2) as rep_month, extract(year from a.dob) as dob_yr\n",
    "    FROM in_fssa.person_month a\n",
    "    INNER JOIN in_fssa.case_month b on a.caseid = b.caseid\n",
    "    WHERE \n",
    "    a.affil = '1' and\n",
    "    a.tanf_end = TRUE and \n",
    "    ssn not in REDACTED and\n",
    "    substring(a.month,1,4) = '2016' and \n",
    "    substring(a.month,5,2) in ('10','11','12')\n",
    "    order by a.ssn, a.month desc;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see cohort_2016 table\n",
    "qry = \"\n",
    "select *\n",
    "from ada_tdc_2020.cohort_2016\n",
    "limit 5\n",
    "\"\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can join our cohort to the wage table in SQL to obtain earnings for four quarters after exiting the TANF program. We will need to perform a `LEFT JOIN` from `cohort_2016` onto `wage_by_employer`, since we only want wage records for those in our cohort, and we will be able to do so through a common `ssn` across the two tables. We will also select the following information:\n",
    "- employer (`uiacct`)\n",
    "- earnings (`wages`)\n",
    "- industry of their employer (`naics_3_digit`)\n",
    "- county of their employment (`cnty`)\n",
    "- first date of quarter (`job_yr_q`)\n",
    "\n",
    "Let's test some code to see if it appears that our join works as intended.\n",
    "\n",
    "> The `format()` function in SQL will allow us to convert `year` and `quarter` combinations into dates. We are listing their job date as the first day of those quarters (i.e. Q2 corresponds to April 1). We won't be using this column in this notebook, but will leverage it in a later one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link cohort to ui wage records to get employment outcomes\n",
    "qry = \"\n",
    "select a.ssn, a.tanf_spell_months, a.tanf_total_months, a.county,\n",
    "b.year, b.quarter, b.uiacct, b.wages, b.naics_3_digit, b.cnty, format('%s-%s-1', b.year, b.quarter*3-2)::date as job_yr_q\n",
    "from ada_tdc_2020.cohort_2016 a\n",
    "left join in_dwd.wage_by_employer b\n",
    "on a.ssn = b.ssn\n",
    "where b.year = 2017\n",
    "limit 5\n",
    "\"\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link cohort to ui wage records to get employment outcomes\n",
    "qry = \"\n",
    "select a.ssn, a.tanf_spell_months, a.tanf_total_months, a.county,\n",
    "b.year, b.quarter, b.uiacct, b.wages, b.naics_3_digit, b.cnty, format('%s-%s-1', b.year, b.quarter*3-2)::date as job_yr_q\n",
    "from ada_tdc_2020.cohort_2016 a\n",
    "left join in_dwd.wage_by_employer b\n",
    "on a.ssn = b.ssn\n",
    "where b.year = 2017\n",
    "limit 5\n",
    "\"\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the code above works, we used it to create a permanent table containing wage outcomes within one year of exit for our cohort. For your viewing pleasure, we included the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    create table ada_tdc_2020.cohort_2016_earnings as\n",
    "    select a.ssn, a.tanf_spell_months, a.tanf_total_months, a.county,\n",
    "    b.year, b.quarter, b.uiacct, b.wages, b.naics_3_digit, b.cnty, \n",
    "    format('%s-%s-1', b.year, b.quarter*3-2)::date as job_yr_q\n",
    "    from ada_tdc_2020.cohort_2016 a\n",
    "    left join in_dwd.wage_by_employer b\n",
    "    on a.ssn = b.ssn\n",
    "    where b.year = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see cohort_2016_earnings\n",
    "qry = \"\n",
    "select *\n",
    "from ada_tdc_2020.cohort_2016_earnings\n",
    "limit 5\n",
    "\"\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 5: Recreate for 2009Q1 </h3></font> \n",
    "\n",
    "Select five rows after joining the 2009Q1 cohort to the UI wage records up to a year after exit by joining `ada_tdc_2020.cohort_2009` to `in_dwd.wage_by_employer`. What did you have to change from the code used above to select 5 rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select five rows from ada_tdc_2020.cohort_2009_wages by joining ada_tdc_2020.cohort_2009 to in_dwd.wage_by_employer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-TANF Employment Outcomes\n",
    "\n",
    "Now, using our new table, `ada_tdc_2020.cohort_2016_earnings`, we will try to get a better sense of where TANF recipients are finding employment and their subsequent earnings by answering specific questions:\n",
    "\n",
    "1. How many leavers found employment in at least one quarter the following year after exit? What percentage is this of our original cohort?\n",
    "1. What were their annualized earnings? What about their average earnings per quarter?\n",
    "1. What were the most popular industries of employment? Do average quarterly earnings per person vary significantly amongst these industries?\n",
    "1. How many different employers did they have in this time frame? In how many quarters were they employed? In how many different counties were they employed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Guiding Question 1: How many leavers found employment in at least one quarter the following year after exit? What percentage is this of our original cohort?</h4></font> \n",
    "\n",
    "First, let's read our `ada_tdc_2020.cohort_2016_earnings` table into R as `df_2016_wages`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read table into R\n",
    "qry = \"\n",
    "select *\n",
    "from ada_tdc_2020.cohort_2016_earnings\n",
    "\"\n",
    "df_2016_wages = dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the amount of leavers that found employment in at least one quarter over the course of this time frame, we can simply count the number of unique `ssn` values. The `tidyverse's` `n_distinct()` function makes that really easy for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of leavers who found employment in at least one quarter \n",
    "df_2016_wages %>%\n",
    "    summarize(n = n_distinct(ssn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to find the percentage of our total cohort, we can leverage `df_2016`, since it contains information on our entire cohort, as well as introduce another function, `percent`, that outputs a percentage as a character vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of our cohort that was employed during at least one quarter the following year after exit\n",
    "percent(n_distinct(df_2016_wages$ssn) / n_distinct(df_2016$ssn), .01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you suprised by this percentage? How do you think it will differ for the 2009Q1 cohort?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Guiding Question 2: What were their annualized earnings? What about their average earnings per quarter?</h4></font> \n",
    "\n",
    "To calculate annualized earnings for each `ssn` across all jobs, we will simply add all of their earnings within this year. \n",
    "\n",
    "> Because (as you will see) a large portion of our cohort wasn't employed all four quarters, we will also look at the average earnings per quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if annualized earnings were calculated properly\n",
    "df_2016_wages %>%\n",
    "    group_by(ssn) %>%\n",
    "    summarize(total_wages = sum(wages)) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save annualized earnings as wages_one_year\n",
    "wages_one_year <- df_2016_wages %>%\n",
    "    group_by(ssn) %>%\n",
    "    summarize(total_wages = sum(wages)) %>%\n",
    "    ungroup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can find a numerical summary of these annualized earnings using both `summary()` and `summarize()` combined with `quantile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick numerical distribution of total_wages\n",
    "summary(wages_one_year$total_wages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more nuanced look at annual wage distribution\n",
    "wages_one_year %>%\n",
    "    summarize('.1' = quantile(total_wages, .1),\n",
    "              '.25' = quantile(total_wages, .25),\n",
    "              '.5' = quantile(total_wages, .5),\n",
    "              '.75' = quantile(total_wages, .75),\n",
    "              '.9' = quantile(total_wages, .9)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you might be wondering why you haven't seen a visual representation of these earnings. Don't worry, you will get to data visualization in the next [notebook](04_01_Data_Visualization.ipynb).\n",
    "\n",
    "Let's take a look at the quarterly wage distribution by adding `quarter` as an argument to our `group_by()` statement to see if we can learn more about our cohort's wage distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if we can find quarterly earnings\n",
    "df_2016_wages %>%\n",
    "    group_by(ssn, quarter) %>%\n",
    "    summarize(quarterly_wages = sum(wages)) %>%\n",
    "    ungroup() %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save quarterly earnings as wages_quarter\n",
    "wages_quarter <- df_2016_wages %>%\n",
    "    group_by(ssn, quarter) %>%\n",
    "    summarize(quarterly_wages = sum(wages)) %>%\n",
    "    ungroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick numerical distribution of quarterly_wages\n",
    "summary(wages_quarter$quarterly_wages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more nuanced look at quarterly earnings distribution\n",
    "wages_quarter %>%\n",
    "    summarize('.1' = quantile(quarterly_wages, .1),\n",
    "              '.25' = quantile(quarterly_wages, .25),\n",
    "              '.5' = quantile(quarterly_wages, .5),\n",
    "              '.75' = quantile(quarterly_wages, .75),\n",
    "              '.9' = quantile(quarterly_wages, .9)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think looking at quarterly or annual wage distributions tells you more about this cohort's employment outcomes? Why? Is there anything else you would like to know about this cohort before answering this question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Guiding Question 3: What were the most popular industries of employment? Do average quarterly earnings per person vary significantly amongst these industries?</h4></font> \n",
    "\n",
    "For this question, we will focus on the 10 most common industries. At this point, you may have started to notice some similar patterns in `tidyverse` chains of commands in discovering certain attributes about our population in question.\n",
    "\n",
    "> The second question infers that we will sum each individual's earnings by quarter within a specific industry. Therefore, if someone worked two jobs in the same industry in the same quarter, the earnings from these two jobs would be combined for the quarterly representation of quarterly earnings for this individual in this industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see if we can find 10 most common naics\n",
    "df_2016_wages %>%\n",
    "    group_by(naics_3_digit) %>%\n",
    "    summarize(num = n_distinct(ssn)) %>% \n",
    "    arrange(desc(num)) %>%\n",
    "    head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save 10 most common naics as pop_naics\n",
    "pop_naics = df_2016_wages %>%\n",
    "    group_by(naics_3_digit) %>%\n",
    "    summarize(num = n_distinct(ssn)) %>% \n",
    "    arrange(desc(num)) %>%\n",
    "    head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although finding these 3-digit NAICS codes is great, it would be far more helpful if we knew the industries corresponding to these NAICS codes. Luckily, we have a table `naics_2017` in the `public` schema that can function as a crosswalk for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the naics_2017 table\n",
    "qry = '\n",
    "select *\n",
    "from public.naics_2017\n",
    "limit 5\n",
    "'\n",
    "dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read naics_2017 table into R as naics\n",
    "qry = '\n",
    "select *\n",
    "from public.naics_2017\n",
    "'\n",
    "naics = dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how we performed a `LEFT JOIN` to match our cohort to the UI wage records, we can follow a similar process here. Instead of using SQL, though, we will use the `tidyverse's` `left_join` function. \n",
    "> Instead of using `on` like in SQL, you need to supply an argument to `by` to designate the columns you would like to join on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get industry names of most popular naics\n",
    "pop_naics %>% \n",
    "    left_join(naics, by=c('naics_3_digit' = 'naics_us_code')) %>%\n",
    "    # don't include the other columns\n",
    "    select(-c(seq_no,naics_3_digit)) %>%\n",
    "    # sort order of columns\n",
    "    select(naics_us_title, num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to find the average earnings of members of our cohort employed in these industries, we can subset `df_2016_wages` using the `tidyverse's` `filter()` function to just include wage records for those employed in the 10 most popular industries.\n",
    "\n",
    "> `%in%` works in the same fashion as SQL's `in`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we can get ssn, earnings, naics and quarter for all wage records in our 10 most popular naics codes\n",
    "df_2016_wages %>%\n",
    "    filter(naics_3_digit %in% pop_naics$naics_3_digit) %>%\n",
    "    select(ssn, wages, naics_3_digit, quarter) %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign wage records for our 2016Q4 cohort for 10 most popular naics to wages_pop_naics\n",
    "wages_pop_naics <- df_2016_wages %>%\n",
    "    filter(naics_3_digit %in% pop_naics$naics_3_digit) %>%\n",
    "    select(ssn, wages, naics_3_digit, quarter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we are finding quarterly earnings for each ssn, quarter, naics code combination\n",
    "wages_pop_naics %>%\n",
    "    group_by(ssn, quarter, naics_3_digit) %>%\n",
    "    summarize(tot_wages = sum(wages)) %>%\n",
    "    ungroup() %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to quarterly_naics\n",
    "quarterly_naics <- wages_pop_naics %>%\n",
    "    group_by(ssn, quarter, naics_3_digit) %>%\n",
    "    summarize(tot_wages = sum(wages)) %>%\n",
    "    ungroup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can group `quarterly_naics` by `naics_3_digit` to find average quarterly earnings per `ssn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find average quarterly earnings by industry and include number of people employed at least one quarter in each industry\n",
    "quarterly_naics %>%\n",
    "    group_by(naics_3_digit) %>%\n",
    "    summarize(avg_wages = mean(tot_wages),\n",
    "             num_ssns = n_distinct(ssn)) %>%\n",
    "    arrange(desc(num_ssns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add the cherry on top, we can add in the industry names corresponding to each NAICS code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results from above to pop_naics_wages\n",
    "pop_naics_wages <- quarterly_naics %>%\n",
    "    group_by(naics_3_digit) %>%\n",
    "    summarize(avg_wages = mean(tot_wages),\n",
    "             num_ssns = n_distinct(ssn)) %>%\n",
    "    arrange(desc(num_ssns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in industry names\n",
    "pop_naics_wages %>% \n",
    "    left_join(naics, by=c('naics_3_digit' = 'naics_us_code')) %>%\n",
    "    # don't include the other columns\n",
    "    select(-c(seq_no,naics_3_digit)) %>%\n",
    "    # switch order of columns\n",
    "    select(naics_us_title, avg_wages, num_ssns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you suprised by these results at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green><h4>Guiding Question 4: How many different employers did they have in this time frame? In how many quarters were they employed? In how many different counties were they employed?</h4></font> \n",
    "\n",
    "Yes, answering three questions in a tiny section may seem daunting. However, we can answer these three questions from the same data frame. To create that data frame, we will aggregate the number of employers, quarters of employment, and counties by each `ssn` in `df_2016_wages` using `n_distinct()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we can find amount of diff employers, amount of quarters worked and amount of counties by ssn\n",
    "df_2016_wages %>%\n",
    "    group_by(ssn) %>%\n",
    "    summarize(num_employers = n_distinct(uiacct),\n",
    "              quarters_work = n_distinct(quarter),\n",
    "              counties_work = n_distinct(cnty)\n",
    "    ) %>%\n",
    "    ungroup() %>%\n",
    "    head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as \"stats\"\n",
    "stats <- df_2016_wages %>%\n",
    "    group_by(ssn) %>%\n",
    "    summarize(num_employers = n_distinct(uiacct),\n",
    "              quarters_work = n_distinct(quarter),\n",
    "              counties_work = n_distinct(cnty)\n",
    "    ) %>%\n",
    "    ungroup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our base data frame to answer this set of questions, we can separately group by `num_employers`, `quarters_work`, and `counties_work`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see number of ssns by amount of employers\n",
    "stats %>%\n",
    "    group_by(num_employers) %>%\n",
    "    summarize(n=n())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you suprised by this result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of quarters worked per ssn\n",
    "stats %>%\n",
    "    group_by(quarters_work) %>%\n",
    "    summarize(n=n())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a better look of why it may be more accurate to report quarterly earnings as opposed to annual ones (not everyone was working all four quarters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total counties worked in\n",
    "stats %>%\n",
    "    group_by(counties_work) %>%\n",
    "    summarize(n=n())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, while we are looking at the amount of counties members of our cohort worked in during this time frame, we can also find the most popular counties of work in Indiana from `df_2016_wages`.\n",
    "\n",
    "One thing to note from the data dictionary on the UI wage records: There are missing county codes--these pertain to entries with values `999`, `995`, or `900`. We'll filter these ones from our analysis here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most popular counties\n",
    "pop_cntys <- df_2016_wages %>%\n",
    "    filter(!(cnty %in% c(900, 955, 999))) %>%\n",
    "    group_by(cnty) %>%\n",
    "    summarize(n = n_distinct(ssn)) %>%\n",
    "    arrange(desc(n)) %>%\n",
    "    head(10)\n",
    "\n",
    "pop_cntys %>%\n",
    "    left_join(counties, c(\"cnty\"=\"county\")) %>%\n",
    "    select(name, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to the most popular counties to receive TANF assistance for our cohort?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><h3> Checkpoint 6: Recreate for 2009Q1 </h3></font> \n",
    "\n",
    "Find the answers to the questions at the beginning of this section \"Post-TANF Employment Outcomes\" for the 2009Q1 cohort. \n",
    "\n",
    "Do the calculations vary significantly from the ones for our 2016Q4 cohort? Did a larger percentage of our 2009Q1 cohort appear in the UI wage records? How can you compare and contrast calculations for the other questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many leavers found employment in at least one quarter the following year after exit? \n",
    "# What percentage is this of our original cohort?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What were their annualized earnings? \n",
    "# What about their average earnings per quarter?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What were the most popular industries of employment?\n",
    "# Do average quarterly earnings per person vary significantly amongst these industries?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many different employers did they have in this time frame? \n",
    "# In how many quarters were they employed? \n",
    "# In how many different counties were they employed?\n",
    "# What were the most popular counties of employment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you have covered how to identify the cohort that you are interested in from a database and save it as data frame in R. You have also seen how to conduct descriptive analyses in R, such as checking missing values and breaking down the sample based on the variables that you are interested.\n",
    "\n",
    "After you find interesting results, you may want to present them in the form of pictures, or visualizations. In the next notebook, which will cover [Data Visualization](Data_Visualization.ipynb), we will show you how to leverage more of the `tidyverse` suite of packages in R to display some of your findings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "adrf_r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "566px",
    "left": "0px",
    "right": "954px",
    "top": "110px",
    "width": "179px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
